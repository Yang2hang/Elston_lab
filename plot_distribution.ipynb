{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1e30ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import decomposition\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from matplotlib_venn import venn3\n",
    "import matplotlib.colors as mcolors\n",
    "import warnings\n",
    "import pingouin as pg\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import seaborn as sns\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "\n",
    " \n",
    "warnings.filterwarnings(\"ignore\", message=\"Mean of empty slice\", category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c98de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions\n",
    "def get_labelled_posteriors(indata, labels):\n",
    "\n",
    "    '''\n",
    "    INPUTS:\n",
    "    indata = posterior probabilites from a classifier with the shape\n",
    "            n_trials x n_timesteps x n_classes\n",
    "        \n",
    "    labels = 1d array with len(n_trials) - these labels ought\n",
    "            to correspond to class numbers (layers in indata)\n",
    "\n",
    "    OUTPUT:\n",
    "        labelled_posteriors = posterior probabilities associated with the\n",
    "        classes in the labels input for each timestep and trial\n",
    "    '''\n",
    "\n",
    "    n_trials, n_times, n_classes = indata.shape\n",
    "    class_lbls = np.unique(labels)\n",
    "    class_lbls = class_lbls[~np.isnan(class_lbls)]\n",
    "\n",
    "    # initialize output\n",
    "    labelled_posteriors = np.zeros(shape = (n_trials, n_times))\n",
    "\n",
    "    for ix, lbl in enumerate(class_lbls):\n",
    "        \n",
    "        # find trials where this label was chosen\n",
    "        labelled_posteriors[labels == lbl,:] = indata[labels == lbl,:,int(ix)]\n",
    "        \n",
    "    return labelled_posteriors\n",
    "\n",
    "\n",
    "def pull_balanced_train_set(trials2balance, params2balance):\n",
    "    '''\n",
    "    INPUTS:\n",
    "    trials2balance   - ***logical array*** of the trials you want to balance\n",
    "    params2balance   - ***list*** where each element is a vector of categorical\n",
    "                        parameters to balance (e.g. choice value and side)\n",
    "                        each element of params2balance must have the same\n",
    "                        number of elements as trials2balance\n",
    "    OUTPUTS:\n",
    "    train_ix         - trial indices of a fully balanced training set\n",
    "    leftover_ix      - trial indices of trials not included in train_ix\n",
    "    '''\n",
    "\n",
    "    # Find the indices where trials are selected to balance\n",
    "    balance_indices = np.where(trials2balance)[0]\n",
    "\n",
    "    # Create an array of parameters to balance\n",
    "    params_array = np.array(params2balance).T\n",
    "\n",
    "    # Find unique combinations and their counts\n",
    "    p_combos, p_counts = np.unique(params_array[balance_indices], axis=0, return_counts=True)\n",
    "\n",
    "    # Determine the minimum count for a balanced set\n",
    "    n_to_keep = np.min(p_counts)\n",
    "\n",
    "    # Initialize arrays to mark selected and leftover trials\n",
    "    train_ix = np.zeros(len(trials2balance), dtype=bool)\n",
    "    leftover_ix = np.zeros(len(trials2balance), dtype=bool)\n",
    "\n",
    "    # Select a balanced number of trials for each unique parameter combination\n",
    "    for combo in p_combos:\n",
    "        # Find indices of trials corresponding to the current combination\n",
    "        combo_indices = np.where((params_array == combo).all(axis=1) & trials2balance)[0]\n",
    "\n",
    "        # Shuffle the indices\n",
    "        np.random.shuffle(combo_indices)\n",
    "\n",
    "        # Select n_to_keep trials and mark them as part of the training set\n",
    "        train_ix[combo_indices[:n_to_keep]] = True\n",
    "\n",
    "        # Mark the remaining trials as leftovers\n",
    "        leftover_ix[combo_indices[n_to_keep:]] = True\n",
    "\n",
    "    return train_ix, leftover_ix\n",
    "\n",
    "\n",
    "def random_prop_of_array(inarray, proportion):\n",
    "    '''\n",
    "    INPUTS\n",
    "    inarray = logical/boolean array of indices to potentially use later\n",
    "    proportion = how much of inarray should randomly be selected\n",
    "\n",
    "    OUTPUT\n",
    "    out_array = logical/boolean that's set as 'true' for a proportion of the \n",
    "                initial 'true' values in inarray\n",
    "    '''\n",
    "\n",
    "    out_array = np.zeros(shape = (len(inarray), ))\n",
    "\n",
    "    # find where inarray is true and shuffle those indices\n",
    "    shuffled_ixs = np.random.permutation(np.asarray(np.where(inarray)).flatten())\n",
    "\n",
    "    # keep only a proportion of that array\n",
    "    kept_ix = shuffled_ixs[0: round(len(shuffled_ixs)*proportion)]\n",
    "\n",
    "    # fill in the kept indices\n",
    "    out_array[kept_ix] = 1\n",
    "\n",
    "    # make this a logical/boolean\n",
    "    out_array = out_array > 0\n",
    "\n",
    "    return out_array\n",
    "\n",
    "def find_h5_files(directory):\n",
    "    \"\"\"\n",
    "    Search for HDF5 files (.h5 extension) in the specified directory.\n",
    "\n",
    "    Parameters:\n",
    "    - directory (str): Path to the directory to search for HDF5 files.\n",
    "\n",
    "    Returns:\n",
    "    - List[str]: A list of filenames (including paths) of HDF5 files found in the directory.\n",
    "    \"\"\"\n",
    "    h5_files = []\n",
    "    search_pattern = os.path.join(directory, '*.h5')  # Pattern to search for .h5 files\n",
    "\n",
    "    for file_path in glob.glob(search_pattern):\n",
    "        if os.path.isfile(file_path):\n",
    "            h5_files.append(file_path)\n",
    "\n",
    "    return h5_files\n",
    "\n",
    "def pull_from_h5(file_path, data_to_extract):\n",
    "    try:\n",
    "        with h5py.File(file_path, 'r') as file:\n",
    "            # Check if the data_to_extract exists in the HDF5 file\n",
    "            if data_to_extract in file:\n",
    "                data = file[data_to_extract][...]  # Extract the data\n",
    "                return data\n",
    "            else:\n",
    "                print(f\"'{data_to_extract}' not found in the file.\")\n",
    "                return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "    \n",
    "def list_hdf5_data(file_path):\n",
    "    try:\n",
    "        with h5py.File(file_path, 'r') as file:\n",
    "            print(f\"Datasets in '{file_path}':\")\n",
    "            for dataset in file:\n",
    "                print(dataset)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "def get_ch_and_unch_vals(bhv):\n",
    "    \"\"\"\n",
    "    Extracts chosen (ch_val) and unchosen (unch_val) values associated with each trial.\n",
    "\n",
    "    Parameters:\n",
    "    - bhv (DataFrame): DataFrame behavioral data.\n",
    "\n",
    "    Returns:\n",
    "    - ch_val (ndarray): Array of chosen values for each trial.\n",
    "    - unch_val (ndarray): Array of unchosen values for each trial. \n",
    "                          - places 0s for unchosen values on forced choice trials\n",
    "    \"\"\"\n",
    "    ch_val = np.zeros(shape=(len(bhv, )))\n",
    "    unch_val = np.zeros(shape=(len(bhv, )))\n",
    "\n",
    "    bhv['r_val'] = bhv['r_val'].fillna(0)\n",
    "    bhv['l_val'] = bhv['l_val'].fillna(0)\n",
    "\n",
    "    ch_left = bhv['side'] == -1\n",
    "    ch_right = bhv['side'] == 1\n",
    "\n",
    "    ch_val[ch_left] = bhv['l_val'].loc[ch_left].astype(int)\n",
    "    ch_val[ch_right] = bhv['r_val'].loc[ch_right].astype(int)\n",
    "\n",
    "    unch_val[ch_left] = bhv['r_val'].loc[ch_left].astype(int)\n",
    "    unch_val[ch_right] = bhv['l_val'].loc[ch_right].astype(int)\n",
    "\n",
    "    return ch_val, unch_val\n",
    "\n",
    "\n",
    "def get_ch_and_unch_pps(in_pp, bhv, ch_val, unch_val):\n",
    "    \"\"\"Gets the posteriors associated with the chosen and unchosen classes\n",
    "\n",
    "    Args:\n",
    "        in_pp (ndarray): array of posteriors (n_trials x n_times x n_classes)\n",
    "        bhv (dataframe): details of each trial\n",
    "        ch_val (ndarray): vector indicating the class that is ultimately chosen\n",
    "        unch_val (ndarray): vector indicating the class that was ultimately not chosen\n",
    "\n",
    "    Returns:\n",
    "        ch_pp (ndarray): vector of the postior at each point in time for each trial's chosen option\n",
    "        unch_pp (ndarray): vector of the postior at each point in time for each trial's unchosen option\n",
    "    \"\"\"\n",
    "\n",
    "    # select the chosen and unchosen values \n",
    "    n_trials, n_times, n_classes = np.shape(in_pp)\n",
    "    ch_pp = np.zeros(shape=(n_trials, n_times))\n",
    "    unch_pp = np.zeros(shape=(n_trials, n_times))\n",
    "\n",
    "    # loop over each trial\n",
    "    for t in range(n_trials):\n",
    "        \n",
    "        # get the chosen and unchosen PPs\n",
    "        ch_pp[t, :] = in_pp[t, :, int(ch_val[t]-1)]\n",
    "        unch_pp[t, :] = in_pp[t, :, int(unch_val[t]-1)]\n",
    "        \n",
    "    # set the forced choice unchosen pps to nans, since there was only 1 option\n",
    "    unch_pp[bhv['forced'] == 1, :] = np.nan\n",
    "    \n",
    "    return ch_pp, unch_pp\n",
    "\n",
    "\n",
    "def get_alt_ch_and_unch_pps(in_pp, bhv, s_ch_val, s_unch_val):\n",
    "    \"\"\"Gets the posteriors associated with the chosen and unchosen classes\n",
    "\n",
    "    Args:\n",
    "        in_pp (ndarray): array of posteriors (n_trials x n_times x n_classes)\n",
    "        bhv (dataframe): details of each trial\n",
    "        s_ch_val (ndarray): vector indicating the class that is ultimately chosen\n",
    "        s_unch_val (ndarray): vector indicating the class that was ultimately not chosen\n",
    "\n",
    "    Returns:\n",
    "        alt_ch_pp (ndarray): vector of the postior at each point in time for the alternative value in the other state\n",
    "        alt_unch_pp (ndarray): vector of the postior at each point in time for the alternative value in the other state\n",
    "    \"\"\"\n",
    "\n",
    "    # select the chosen and unchosen values \n",
    "    n_trials, n_times, n_classes = np.shape(in_pp)\n",
    "    alt_ch_pp = np.zeros(shape=(n_trials, n_times))\n",
    "    alt_unch_pp = np.zeros(shape=(n_trials, n_times))\n",
    "\n",
    "    alt_ch_val = np.zeros_like(s_ch_val)\n",
    "    alt_unch_val = np.zeros_like(s_unch_val)\n",
    "    \n",
    "    alt_ch_val[bhv['state'] == 1] = 8 - s_ch_val[bhv['state'] == 1] + 1\n",
    "    alt_ch_val[bhv['state'] == 2] = 8 - s_ch_val[bhv['state'] == 2] + 1\n",
    "\n",
    "    alt_unch_val[bhv['state'] == 1] = 8 - s_unch_val[bhv['state'] == 1] + 1\n",
    "    alt_unch_val[bhv['state'] == 2] = 8 - s_unch_val[bhv['state'] == 2] + 1\n",
    "\n",
    "    for t in range(n_trials):\n",
    "        \n",
    "        alt_ch_pp[t, :] = in_pp[t, :, int(alt_ch_val[t]-1)]\n",
    "        alt_unch_pp[t, :] = in_pp[t, :, int(alt_unch_val[t]-1)]\n",
    "\n",
    "    # set the alternative values to nans for state 3, since there were no alternatives\n",
    "    alt_ch_pp[bhv['state'] == 3] = np.nan\n",
    "    alt_unch_pp[bhv['state'] == 3] = np.nan\n",
    "\n",
    "    return alt_ch_pp, alt_unch_pp\n",
    "\n",
    "def find_candidate_states(indata, n_classes, temporal_thresh, mag_thresh):\n",
    "    \"\"\"Finds periods where decoded posteriors are twice their noise level.\n",
    "\n",
    "    Args:\n",
    "        indata (ndarray): 2d array of posterior probabilities associated with some decoder output.\n",
    "        n_classes (int): How many classes were used in the decoder?\n",
    "        temporal_thresh (int): Number of contiguous samples that must be above a threshold to be a real state (typically 2).\n",
    "        mag_thresh (flat): how many times the noise level must a state be? (e.g. 2 = twice the noise level)\n",
    "\n",
    "    Returns:\n",
    "        state_details (ndarray): 2d array where each row details when a state occurred [trial_num, time_in_trial, state_length].\n",
    "        state_array (ndarray): 2d array the same size as indata. It contains 1 in all locations where there were states and 0s everywhere else.\n",
    "    \"\"\"\n",
    "    state_details = np.array([])\n",
    "    state_array = np.zeros_like(indata)\n",
    "    \n",
    "    state_magnitude_thresh = (1 / n_classes) * mag_thresh\n",
    "\n",
    "    for t in range(indata.shape[0]):\n",
    "        state_len, state_pos, state_type = find_1dsequences(indata[t, :] > state_magnitude_thresh)\n",
    "        state_len = state_len[state_type == True]\n",
    "        state_pos = state_pos[state_type == True]\n",
    "\n",
    "        for i in range(len(state_len)):\n",
    "            state_details = np.concatenate((state_details, np.array([t, state_pos[i], state_len[i]])))\n",
    "\n",
    "    state_details = state_details.reshape(-1, 3)\n",
    "    state_details = state_details[state_details[:, 2] > temporal_thresh, :]\n",
    "\n",
    "    # Update state_array using state_details information\n",
    "    for j in range(len(state_details)):\n",
    "        state_trial, state_start, state_len = state_details[j].astype(int)\n",
    "        state_array[state_trial, state_start:(state_start + state_len)] = 1\n",
    "\n",
    "    return state_details, state_array\n",
    "\n",
    "def moving_average(x, w, axis=0):\n",
    "    '''\n",
    "    Moving average function that operates along specified dimensions of a NumPy array.\n",
    "\n",
    "    Parameters:\n",
    "    - x (numpy.ndarray): Input array.\n",
    "    - w (int): Size of the window to convolve the array with (i.e., smoothness factor).\n",
    "    - axis (int): Axis along which to perform the moving average (default is 0).\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: Smoothed array along the specified axis with the same size as the input array.\n",
    "    '''\n",
    "    x = np.asarray(x)  # Ensure input is a NumPy array\n",
    "    if np.isnan(x).any():\n",
    "        x = np.nan_to_num(x)  # Replace NaN values with zeros\n",
    "\n",
    "    if axis < 0:\n",
    "        axis += x.ndim  # Adjust negative axis value\n",
    "\n",
    "    kernel = np.ones(w) / w  # Create kernel for moving average\n",
    "\n",
    "    # Pad the array before applying convolution\n",
    "    pad_width = [(0, 0)] * x.ndim  # Initialize padding for each axis\n",
    "    pad_width[axis] = (w - 1, 0)  # Pad along the specified axis (left side)\n",
    "    x_padded = np.pad(x, pad_width, mode='constant', constant_values=0)\n",
    "\n",
    "    # Apply 1D convolution along the specified axis on the padded array\n",
    "    return np.apply_along_axis(lambda m: np.convolve(m, kernel, mode='valid'), axis, x_padded)\n",
    "\n",
    "def find_1dsequences(inarray):\n",
    "        ''' \n",
    "        run length encoding. Partial credit to R rle function. \n",
    "        Multi datatype arrays catered for including non Numpy\n",
    "        returns: tuple (runlengths, startpositions, values) \n",
    "        '''\n",
    "        ia = np.asarray(inarray)                # force numpy\n",
    "        n = len(ia)\n",
    "        if n == 0: \n",
    "            return (None, None, None)\n",
    "        else:\n",
    "            y = ia[1:] != ia[:-1]                 # pairwise unequal (string safe)\n",
    "            i = np.append(np.where(y), n - 1)     # must include last element \n",
    "            lens = np.diff(np.append(-1, i))      # run lengths\n",
    "            pos = np.cumsum(np.append(0, lens))[:-1] # positions\n",
    "            return(lens, pos, ia[i])\n",
    "        \n",
    "        \n",
    "def calculate_mean_and_interval(data, type='sem', num_samples=1000, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Calculate mean and either SEM or bootstrapped CI for each column of the input array, disregarding NaN values.\n",
    "\n",
    "    Parameters:\n",
    "    - data: 2D numpy array\n",
    "    - type: str, either 'sem' or 'bootstrap_ci'\n",
    "    - num_samples: int, number of bootstrap samples (applicable only for type='bootstrap_ci')\n",
    "    - alpha: float, significance level for the confidence interval (applicable only for type='bootstrap_ci')\n",
    "\n",
    "    Returns:\n",
    "    - means: 1D numpy array containing means for each column\n",
    "    - interval: 1D numpy array containing SEMs or bootstrapped CIs for each column\n",
    "    \"\"\"\n",
    "    nan_mask = ~np.isnan(data)\n",
    "    \n",
    "    nanmean_result = np.nanmean(data, axis=0)\n",
    "    n_valid_values = np.sum(nan_mask, axis=0)\n",
    "    \n",
    "    if type == 'sem':\n",
    "        nanstd_result = np.nanstd(data, axis=0)\n",
    "        interval = nanstd_result / np.sqrt(n_valid_values)\n",
    "        \n",
    "    elif type == 'percentile':\n",
    "        interval = np.mean(np.array([np.abs(nanmean_result - np.nanpercentile (data, 5, axis=0)), np.abs(nanmean_result - np.nanpercentile (data, 95, axis=0))]))\n",
    "        \n",
    "        \n",
    "    elif type == 'bootstrap':\n",
    "        n_rows, n_cols = data.shape\n",
    "\n",
    "        # Initialize array to store bootstrap means\n",
    "        bootstrap_means = np.zeros((num_samples, n_cols))\n",
    "\n",
    "        # Perform bootstrap resampling for each column\n",
    "        for col in range(n_cols):\n",
    "            bootstrap_samples = np.random.choice(data[:, col][nan_mask[:, col]], size=(num_samples, n_rows), replace=True)\n",
    "            bootstrap_means[:, col] = np.mean(bootstrap_samples, axis=1)\n",
    "\n",
    "        # Calculate confidence interval bounds\n",
    "        ci_lower = np.percentile(bootstrap_means, 100 * (alpha / 2), axis=0)\n",
    "        ci_upper = np.percentile(bootstrap_means, 100 * (1 - alpha / 2), axis=0)\n",
    "        \n",
    "        interval = np.mean([abs(bootstrap_means - ci_lower), abs(bootstrap_means - ci_upper)], axis=0)\n",
    "        \n",
    "        interval = np.mean(interval, axis=0)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Invalid 'type' argument. Use either 'sem' or 'bootstrap'.\")\n",
    "    \n",
    "    return nanmean_result, interval\n",
    "\n",
    "def get_value_correlation(row):\n",
    "    if row['coding_type'] == 'value':\n",
    "        return np.sign(row['value_beta'])\n",
    "    \n",
    "    elif row['coding_type'] == 'state':\n",
    "        state_betas = [row['state_1_beta'], row['state_2_beta'], row['state_3_beta']]\n",
    "        max_idx = np.argmax(np.abs(state_betas))\n",
    "        return np.sign(state_betas[max_idx])\n",
    "    \n",
    "    elif row['coding_type'] == 'state_value':\n",
    "        val_state_betas = [row['state_value_1_beta'], row['state_value_2_beta'], row['state_value_3_beta']]\n",
    "        max_idx = np.argmax(np.abs(val_state_betas))\n",
    "        return np.sign(val_state_betas[max_idx])\n",
    "    \n",
    "    else:\n",
    "        return 'none'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5680acbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = r\"D:\\Projects\\rotation_project\\reprocessed_data\"\n",
    "data_files = find_h5_files(datadir)\n",
    "\n",
    "all_neurons = []\n",
    "\n",
    "for file_path in data_files:\n",
    "    session_name = os.path.basename(file_path).replace('.h5', '')\n",
    "\n",
    "    # Load data\n",
    "    CdN_zFR = pull_from_h5(file_path, 'CdN_zFR')\n",
    "    OFC_zFR = pull_from_h5(file_path, 'OFC_zFR')\n",
    "    firing_rates = np.concatenate([CdN_zFR, OFC_zFR], axis=2)\n",
    "\n",
    "    n_CdN = CdN_zFR.shape[2]\n",
    "    n_OFC = OFC_zFR.shape[2]\n",
    "    brain_areas = np.concatenate([np.zeros(n_CdN), np.ones(n_OFC)]).astype(int)\n",
    "\n",
    "    u_locations = np.concatenate([\n",
    "        pull_from_h5(file_path, 'CdN_locations'),\n",
    "        pull_from_h5(file_path, 'OFC_locations')\n",
    "    ], axis=0)\n",
    "\n",
    "    ts = pull_from_h5(file_path, 'ts')\n",
    "    bhv = pd.read_hdf(file_path, key='bhv')\n",
    "\n",
    "    if len(bhv) > len(firing_rates):\n",
    "        bhv = bhv.iloc[:len(firing_rates)]\n",
    "\n",
    "    trials2keep = (bhv['n_sacc'] > 0)\n",
    "    bhv = bhv.loc[trials2keep]\n",
    "    firing_rates = firing_rates[trials2keep, :, :]\n",
    "    firing_rates = np.nan_to_num(firing_rates, nan=0)\n",
    "\n",
    "    all_neurons.append(session_df)\n",
    "    \n",
    "# Combine all sessions into one big DataFrame\n",
    "neuron_profile_all = pd.concat(all_neurons, ignore_index=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "elstonlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
