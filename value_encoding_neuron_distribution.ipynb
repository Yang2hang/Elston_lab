{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Encoding Neuron Distribution\n",
    "Assesses data from neuropixels recordings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import decomposition\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from matplotlib_venn import venn3\n",
    "import matplotlib.colors as mcolors\n",
    "import warnings\n",
    "import pingouin as pg\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import seaborn as sns\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "\n",
    " \n",
    "warnings.filterwarnings(\"ignore\", message=\"Mean of empty slice\", category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions\n",
    "def get_labelled_posteriors(indata, labels):\n",
    "\n",
    "    '''\n",
    "    INPUTS:\n",
    "    indata = posterior probabilites from a classifier with the shape\n",
    "            n_trials x n_timesteps x n_classes\n",
    "        \n",
    "    labels = 1d array with len(n_trials) - these labels ought\n",
    "            to correspond to class numbers (layers in indata)\n",
    "\n",
    "    OUTPUT:\n",
    "        labelled_posteriors = posterior probabilities associated with the\n",
    "        classes in the labels input for each timestep and trial\n",
    "    '''\n",
    "\n",
    "    n_trials, n_times, n_classes = indata.shape\n",
    "    class_lbls = np.unique(labels)\n",
    "    class_lbls = class_lbls[~np.isnan(class_lbls)]\n",
    "\n",
    "    # initialize output\n",
    "    labelled_posteriors = np.zeros(shape = (n_trials, n_times))\n",
    "\n",
    "    for ix, lbl in enumerate(class_lbls):\n",
    "        \n",
    "        # find trials where this label was chosen\n",
    "        labelled_posteriors[labels == lbl,:] = indata[labels == lbl,:,int(ix)]\n",
    "        \n",
    "    return labelled_posteriors\n",
    "\n",
    "\n",
    "def pull_balanced_train_set(trials2balance, params2balance):\n",
    "    '''\n",
    "    INPUTS:\n",
    "    trials2balance   - ***logical array*** of the trials you want to balance\n",
    "    params2balance   - ***list*** where each element is a vector of categorical\n",
    "                        parameters to balance (e.g. choice value and side)\n",
    "                        each element of params2balance must have the same\n",
    "                        number of elements as trials2balance\n",
    "    OUTPUTS:\n",
    "    train_ix         - trial indices of a fully balanced training set\n",
    "    leftover_ix      - trial indices of trials not included in train_ix\n",
    "    '''\n",
    "\n",
    "    # Find the indices where trials are selected to balance\n",
    "    balance_indices = np.where(trials2balance)[0]\n",
    "\n",
    "    # Create an array of parameters to balance\n",
    "    params_array = np.array(params2balance).T\n",
    "\n",
    "    # Find unique combinations and their counts\n",
    "    p_combos, p_counts = np.unique(params_array[balance_indices], axis=0, return_counts=True)\n",
    "\n",
    "    # Determine the minimum count for a balanced set\n",
    "    n_to_keep = np.min(p_counts)\n",
    "\n",
    "    # Initialize arrays to mark selected and leftover trials\n",
    "    train_ix = np.zeros(len(trials2balance), dtype=bool)\n",
    "    leftover_ix = np.zeros(len(trials2balance), dtype=bool)\n",
    "\n",
    "    # Select a balanced number of trials for each unique parameter combination\n",
    "    for combo in p_combos:\n",
    "        # Find indices of trials corresponding to the current combination\n",
    "        combo_indices = np.where((params_array == combo).all(axis=1) & trials2balance)[0]\n",
    "\n",
    "        # Shuffle the indices\n",
    "        np.random.shuffle(combo_indices)\n",
    "\n",
    "        # Select n_to_keep trials and mark them as part of the training set\n",
    "        train_ix[combo_indices[:n_to_keep]] = True\n",
    "\n",
    "        # Mark the remaining trials as leftovers\n",
    "        leftover_ix[combo_indices[n_to_keep:]] = True\n",
    "\n",
    "    return train_ix, leftover_ix\n",
    "\n",
    "\n",
    "def random_prop_of_array(inarray, proportion):\n",
    "    '''\n",
    "    INPUTS\n",
    "    inarray = logical/boolean array of indices to potentially use later\n",
    "    proportion = how much of inarray should randomly be selected\n",
    "\n",
    "    OUTPUT\n",
    "    out_array = logical/boolean that's set as 'true' for a proportion of the \n",
    "                initial 'true' values in inarray\n",
    "    '''\n",
    "\n",
    "    out_array = np.zeros(shape = (len(inarray), ))\n",
    "\n",
    "    # find where inarray is true and shuffle those indices\n",
    "    shuffled_ixs = np.random.permutation(np.asarray(np.where(inarray)).flatten())\n",
    "\n",
    "    # keep only a proportion of that array\n",
    "    kept_ix = shuffled_ixs[0: round(len(shuffled_ixs)*proportion)]\n",
    "\n",
    "    # fill in the kept indices\n",
    "    out_array[kept_ix] = 1\n",
    "\n",
    "    # make this a logical/boolean\n",
    "    out_array = out_array > 0\n",
    "\n",
    "    return out_array\n",
    "\n",
    "def find_h5_files(directory):\n",
    "    \"\"\"\n",
    "    Search for HDF5 files (.h5 extension) in the specified directory.\n",
    "\n",
    "    Parameters:\n",
    "    - directory (str): Path to the directory to search for HDF5 files.\n",
    "\n",
    "    Returns:\n",
    "    - List[str]: A list of filenames (including paths) of HDF5 files found in the directory.\n",
    "    \"\"\"\n",
    "    h5_files = []\n",
    "    search_pattern = os.path.join(directory, '*.h5')  # Pattern to search for .h5 files\n",
    "\n",
    "    for file_path in glob.glob(search_pattern):\n",
    "        if os.path.isfile(file_path):\n",
    "            h5_files.append(file_path)\n",
    "\n",
    "    return h5_files\n",
    "\n",
    "def pull_from_h5(file_path, data_to_extract):\n",
    "    try:\n",
    "        with h5py.File(file_path, 'r') as file:\n",
    "            # Check if the data_to_extract exists in the HDF5 file\n",
    "            if data_to_extract in file:\n",
    "                data = file[data_to_extract][...]  # Extract the data\n",
    "                return data\n",
    "            else:\n",
    "                print(f\"'{data_to_extract}' not found in the file.\")\n",
    "                return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "    \n",
    "def list_hdf5_data(file_path):\n",
    "    try:\n",
    "        with h5py.File(file_path, 'r') as file:\n",
    "            print(f\"Datasets in '{file_path}':\")\n",
    "            for dataset in file:\n",
    "                print(dataset)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "def get_ch_and_unch_vals(bhv):\n",
    "    \"\"\"\n",
    "    Extracts chosen (ch_val) and unchosen (unch_val) values associated with each trial.\n",
    "\n",
    "    Parameters:\n",
    "    - bhv (DataFrame): DataFrame behavioral data.\n",
    "\n",
    "    Returns:\n",
    "    - ch_val (ndarray): Array of chosen values for each trial.\n",
    "    - unch_val (ndarray): Array of unchosen values for each trial. \n",
    "                          - places 0s for unchosen values on forced choice trials\n",
    "    \"\"\"\n",
    "    ch_val = np.zeros(shape=(len(bhv, )))\n",
    "    unch_val = np.zeros(shape=(len(bhv, )))\n",
    "\n",
    "    bhv['r_val'] = bhv['r_val'].fillna(0)\n",
    "    bhv['l_val'] = bhv['l_val'].fillna(0)\n",
    "\n",
    "    ch_left = bhv['side'] == -1\n",
    "    ch_right = bhv['side'] == 1\n",
    "\n",
    "    ch_val[ch_left] = bhv['l_val'].loc[ch_left].astype(int)\n",
    "    ch_val[ch_right] = bhv['r_val'].loc[ch_right].astype(int)\n",
    "\n",
    "    unch_val[ch_left] = bhv['r_val'].loc[ch_left].astype(int)\n",
    "    unch_val[ch_right] = bhv['l_val'].loc[ch_right].astype(int)\n",
    "\n",
    "    return ch_val, unch_val\n",
    "\n",
    "\n",
    "def get_ch_and_unch_pps(in_pp, bhv, ch_val, unch_val):\n",
    "    \"\"\"Gets the posteriors associated with the chosen and unchosen classes\n",
    "\n",
    "    Args:\n",
    "        in_pp (ndarray): array of posteriors (n_trials x n_times x n_classes)\n",
    "        bhv (dataframe): details of each trial\n",
    "        ch_val (ndarray): vector indicating the class that is ultimately chosen\n",
    "        unch_val (ndarray): vector indicating the class that was ultimately not chosen\n",
    "\n",
    "    Returns:\n",
    "        ch_pp (ndarray): vector of the postior at each point in time for each trial's chosen option\n",
    "        unch_pp (ndarray): vector of the postior at each point in time for each trial's unchosen option\n",
    "    \"\"\"\n",
    "\n",
    "    # select the chosen and unchosen values \n",
    "    n_trials, n_times, n_classes = np.shape(in_pp)\n",
    "    ch_pp = np.zeros(shape=(n_trials, n_times))\n",
    "    unch_pp = np.zeros(shape=(n_trials, n_times))\n",
    "\n",
    "    # loop over each trial\n",
    "    for t in range(n_trials):\n",
    "        \n",
    "        # get the chosen and unchosen PPs\n",
    "        ch_pp[t, :] = in_pp[t, :, int(ch_val[t]-1)]\n",
    "        unch_pp[t, :] = in_pp[t, :, int(unch_val[t]-1)]\n",
    "        \n",
    "    # set the forced choice unchosen pps to nans, since there was only 1 option\n",
    "    unch_pp[bhv['forced'] == 1, :] = np.nan\n",
    "    \n",
    "    return ch_pp, unch_pp\n",
    "\n",
    "\n",
    "def get_alt_ch_and_unch_pps(in_pp, bhv, s_ch_val, s_unch_val):\n",
    "    \"\"\"Gets the posteriors associated with the chosen and unchosen classes\n",
    "\n",
    "    Args:\n",
    "        in_pp (ndarray): array of posteriors (n_trials x n_times x n_classes)\n",
    "        bhv (dataframe): details of each trial\n",
    "        s_ch_val (ndarray): vector indicating the class that is ultimately chosen\n",
    "        s_unch_val (ndarray): vector indicating the class that was ultimately not chosen\n",
    "\n",
    "    Returns:\n",
    "        alt_ch_pp (ndarray): vector of the postior at each point in time for the alternative value in the other state\n",
    "        alt_unch_pp (ndarray): vector of the postior at each point in time for the alternative value in the other state\n",
    "    \"\"\"\n",
    "\n",
    "    # select the chosen and unchosen values \n",
    "    n_trials, n_times, n_classes = np.shape(in_pp)\n",
    "    alt_ch_pp = np.zeros(shape=(n_trials, n_times))\n",
    "    alt_unch_pp = np.zeros(shape=(n_trials, n_times))\n",
    "\n",
    "    alt_ch_val = np.zeros_like(s_ch_val)\n",
    "    alt_unch_val = np.zeros_like(s_unch_val)\n",
    "    \n",
    "    alt_ch_val[bhv['state'] == 1] = 8 - s_ch_val[bhv['state'] == 1] + 1\n",
    "    alt_ch_val[bhv['state'] == 2] = 8 - s_ch_val[bhv['state'] == 2] + 1\n",
    "\n",
    "    alt_unch_val[bhv['state'] == 1] = 8 - s_unch_val[bhv['state'] == 1] + 1\n",
    "    alt_unch_val[bhv['state'] == 2] = 8 - s_unch_val[bhv['state'] == 2] + 1\n",
    "\n",
    "    for t in range(n_trials):\n",
    "        \n",
    "        alt_ch_pp[t, :] = in_pp[t, :, int(alt_ch_val[t]-1)]\n",
    "        alt_unch_pp[t, :] = in_pp[t, :, int(alt_unch_val[t]-1)]\n",
    "\n",
    "    # set the alternative values to nans for state 3, since there were no alternatives\n",
    "    alt_ch_pp[bhv['state'] == 3] = np.nan\n",
    "    alt_unch_pp[bhv['state'] == 3] = np.nan\n",
    "\n",
    "    return alt_ch_pp, alt_unch_pp\n",
    "\n",
    "def find_candidate_states(indata, n_classes, temporal_thresh, mag_thresh):\n",
    "    \"\"\"Finds periods where decoded posteriors are twice their noise level.\n",
    "\n",
    "    Args:\n",
    "        indata (ndarray): 2d array of posterior probabilities associated with some decoder output.\n",
    "        n_classes (int): How many classes were used in the decoder?\n",
    "        temporal_thresh (int): Number of contiguous samples that must be above a threshold to be a real state (typically 2).\n",
    "        mag_thresh (flat): how many times the noise level must a state be? (e.g. 2 = twice the noise level)\n",
    "\n",
    "    Returns:\n",
    "        state_details (ndarray): 2d array where each row details when a state occurred [trial_num, time_in_trial, state_length].\n",
    "        state_array (ndarray): 2d array the same size as indata. It contains 1 in all locations where there were states and 0s everywhere else.\n",
    "    \"\"\"\n",
    "    state_details = np.array([])\n",
    "    state_array = np.zeros_like(indata)\n",
    "    \n",
    "    state_magnitude_thresh = (1 / n_classes) * mag_thresh\n",
    "\n",
    "    for t in range(indata.shape[0]):\n",
    "        state_len, state_pos, state_type = find_1dsequences(indata[t, :] > state_magnitude_thresh)\n",
    "        state_len = state_len[state_type == True]\n",
    "        state_pos = state_pos[state_type == True]\n",
    "\n",
    "        for i in range(len(state_len)):\n",
    "            state_details = np.concatenate((state_details, np.array([t, state_pos[i], state_len[i]])))\n",
    "\n",
    "    state_details = state_details.reshape(-1, 3)\n",
    "    state_details = state_details[state_details[:, 2] > temporal_thresh, :]\n",
    "\n",
    "    # Update state_array using state_details information\n",
    "    for j in range(len(state_details)):\n",
    "        state_trial, state_start, state_len = state_details[j].astype(int)\n",
    "        state_array[state_trial, state_start:(state_start + state_len)] = 1\n",
    "\n",
    "    return state_details, state_array\n",
    "\n",
    "def moving_average(x, w, axis=0):\n",
    "    '''\n",
    "    Moving average function that operates along specified dimensions of a NumPy array.\n",
    "\n",
    "    Parameters:\n",
    "    - x (numpy.ndarray): Input array.\n",
    "    - w (int): Size of the window to convolve the array with (i.e., smoothness factor).\n",
    "    - axis (int): Axis along which to perform the moving average (default is 0).\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: Smoothed array along the specified axis with the same size as the input array.\n",
    "    '''\n",
    "    x = np.asarray(x)  # Ensure input is a NumPy array\n",
    "    if np.isnan(x).any():\n",
    "        x = np.nan_to_num(x)  # Replace NaN values with zeros\n",
    "\n",
    "    if axis < 0:\n",
    "        axis += x.ndim  # Adjust negative axis value\n",
    "\n",
    "    kernel = np.ones(w) / w  # Create kernel for moving average\n",
    "\n",
    "    # Pad the array before applying convolution\n",
    "    pad_width = [(0, 0)] * x.ndim  # Initialize padding for each axis\n",
    "    pad_width[axis] = (w - 1, 0)  # Pad along the specified axis (left side)\n",
    "    x_padded = np.pad(x, pad_width, mode='constant', constant_values=0)\n",
    "\n",
    "    # Apply 1D convolution along the specified axis on the padded array\n",
    "    return np.apply_along_axis(lambda m: np.convolve(m, kernel, mode='valid'), axis, x_padded)\n",
    "\n",
    "def find_1dsequences(inarray):\n",
    "        ''' \n",
    "        run length encoding. Partial credit to R rle function. \n",
    "        Multi datatype arrays catered for including non Numpy\n",
    "        returns: tuple (runlengths, startpositions, values) \n",
    "        '''\n",
    "        ia = np.asarray(inarray)                # force numpy\n",
    "        n = len(ia)\n",
    "        if n == 0: \n",
    "            return (None, None, None)\n",
    "        else:\n",
    "            y = ia[1:] != ia[:-1]                 # pairwise unequal (string safe)\n",
    "            i = np.append(np.where(y), n - 1)     # must include last element \n",
    "            lens = np.diff(np.append(-1, i))      # run lengths\n",
    "            pos = np.cumsum(np.append(0, lens))[:-1] # positions\n",
    "            return(lens, pos, ia[i])\n",
    "        \n",
    "        \n",
    "def calculate_mean_and_interval(data, type='sem', num_samples=1000, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Calculate mean and either SEM or bootstrapped CI for each column of the input array, disregarding NaN values.\n",
    "\n",
    "    Parameters:\n",
    "    - data: 2D numpy array\n",
    "    - type: str, either 'sem' or 'bootstrap_ci'\n",
    "    - num_samples: int, number of bootstrap samples (applicable only for type='bootstrap_ci')\n",
    "    - alpha: float, significance level for the confidence interval (applicable only for type='bootstrap_ci')\n",
    "\n",
    "    Returns:\n",
    "    - means: 1D numpy array containing means for each column\n",
    "    - interval: 1D numpy array containing SEMs or bootstrapped CIs for each column\n",
    "    \"\"\"\n",
    "    nan_mask = ~np.isnan(data)\n",
    "    \n",
    "    nanmean_result = np.nanmean(data, axis=0)\n",
    "    n_valid_values = np.sum(nan_mask, axis=0)\n",
    "    \n",
    "    if type == 'sem':\n",
    "        nanstd_result = np.nanstd(data, axis=0)\n",
    "        interval = nanstd_result / np.sqrt(n_valid_values)\n",
    "        \n",
    "    elif type == 'percentile':\n",
    "        interval = np.mean(np.array([np.abs(nanmean_result - np.nanpercentile (data, 5, axis=0)), np.abs(nanmean_result - np.nanpercentile (data, 95, axis=0))]))\n",
    "        \n",
    "        \n",
    "    elif type == 'bootstrap':\n",
    "        n_rows, n_cols = data.shape\n",
    "\n",
    "        # Initialize array to store bootstrap means\n",
    "        bootstrap_means = np.zeros((num_samples, n_cols))\n",
    "\n",
    "        # Perform bootstrap resampling for each column\n",
    "        for col in range(n_cols):\n",
    "            bootstrap_samples = np.random.choice(data[:, col][nan_mask[:, col]], size=(num_samples, n_rows), replace=True)\n",
    "            bootstrap_means[:, col] = np.mean(bootstrap_samples, axis=1)\n",
    "\n",
    "        # Calculate confidence interval bounds\n",
    "        ci_lower = np.percentile(bootstrap_means, 100 * (alpha / 2), axis=0)\n",
    "        ci_upper = np.percentile(bootstrap_means, 100 * (1 - alpha / 2), axis=0)\n",
    "        \n",
    "        interval = np.mean([abs(bootstrap_means - ci_lower), abs(bootstrap_means - ci_upper)], axis=0)\n",
    "        \n",
    "        interval = np.mean(interval, axis=0)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Invalid 'type' argument. Use either 'sem' or 'bootstrap'.\")\n",
    "    \n",
    "    return nanmean_result, interval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = r\"D:\\Projects\\rotation_project\\reprocessed_data\"\n",
    "data_files = find_h5_files(datadir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'D:\\\\Projects\\\\rotation_project\\\\reprocessed_data\\\\D20231219_Rec05.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# access the data for this session\n",
    "firing_rates = np.concatenate([pull_from_h5(file_path, 'CdN_zFR'), \n",
    "                               pull_from_h5(file_path, 'OFC_zFR')], axis=2)\n",
    "\n",
    "u_names = np.concatenate([pull_from_h5(file_path, 'CdN_u_names'), \n",
    "                          pull_from_h5(file_path, 'OFC_u_names')], axis=0)\n",
    "\n",
    "n_CdN = pull_from_h5(file_path, 'CdN_zFR').shape[2]\n",
    "n_OFC = pull_from_h5(file_path, 'OFC_zFR').shape[2]\n",
    "brain_areas = np.concatenate([np.zeros(shape=n_CdN, ), np.ones(shape=n_OFC, )]).astype(int)\n",
    "\n",
    "u_locations = np.concatenate([pull_from_h5(file_path, 'CdN_locations'), \n",
    "                              pull_from_h5(file_path, 'OFC_locations')], axis=0)\n",
    "\n",
    "ts = pull_from_h5(file_path, 'ts')\n",
    "bhv = pd.read_hdf(file_path, key='bhv')\n",
    "\n",
    "if len(bhv) > len(firing_rates):\n",
    "    bhv = bhv.loc[0 :len(firing_rates)-1]\n",
    "\n",
    "# subselect trials with a response that was correct\n",
    "trials2keep = (bhv['n_sacc'] > 0)\n",
    "bhv = bhv.loc[trials2keep]\n",
    "firing_rates = firing_rates[trials2keep, :,:]\n",
    "firing_rates = np.nan_to_num(firing_rates, nan=0)\n",
    "\n",
    "n_trials, n_times, n_units = np.shape(firing_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"firing_rates shape:\", firing_rates.shape)  # (n_trials, n_times, n_units)\n",
    "print(\"bhv shape:\", bhv.shape)\n",
    "print(\"Number of CdN units:\", n_CdN)\n",
    "print(\"Number of OFC units:\", n_OFC)\n",
    "print('bhv columns:', bhv.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined mask: exactly one saccade AND picked the best option\n",
    "mask = (bhv['n_sacc'] == 1) & (bhv['picked_best'] == 1)\n",
    "\n",
    "# Apply mask\n",
    "trial_profile = bhv[mask]\n",
    "firing_single_best = firing_rates[mask.values, :, :]\n",
    "\n",
    "# Average firing rate across time for each trial and neuron\n",
    "# Shape: (n_trials, n_units)\n",
    "mean_FR = firing_single_best.mean(axis=1)\n",
    "\n",
    "FR_profile = pd.DataFrame(mean_FR, columns=[f'neuron_{i}' for i in range(mean_FR.shape[1])]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GLM to decode neuron tuning profiles\n",
    "import statsmodels.api as sm\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'value': trial_profile['ch_val'].values,\n",
    "    'state': trial_profile['state'].values\n",
    "})\n",
    "\n",
    "state_dummies = pd.get_dummies(df['state'].astype(int), prefix='state')\n",
    "df = pd.concat([df, state_dummies], axis=1)\n",
    "\n",
    "df['value_state_1'] = df['value'] * df['state_1']\n",
    "df['value_state_2'] = df['value'] * df['state_2']\n",
    "df['value_state_3'] = df['value'] * df['state_3']\n",
    "\n",
    "X = df[['value', 'state_1', 'state_2', 'state_3',\n",
    "            'value_state_1', 'value_state_2', 'value_state_3']]\n",
    "\n",
    "X[['state_1', 'state_2', 'state_3']] = X[['state_1', 'state_2', 'state_3']].astype(int)\n",
    "X = sm.add_constant(X)  # Add intercept term\n",
    "\n",
    "value_betas = []\n",
    "value_pvals = []\n",
    "lateral = u_locations[:, 0]\n",
    "depth = u_locations[:, 1]\n",
    "\n",
    "# Initialize containers for beta and p-value\n",
    "beta_dict = {col: [] for col in X.columns}\n",
    "pval_dict = {col: [] for col in X.columns}\n",
    "\n",
    "# Loop through neurons\n",
    "for i in range(n_units):\n",
    "    y = mean_FR[:, i]\n",
    "    model = sm.OLS(y, X).fit()\n",
    "\n",
    "    # Append each beta to its corresponding list\n",
    "    for col in X.columns:\n",
    "        beta_dict[col].append(model.params[col])\n",
    "        pval_dict[col].append(model.pvalues[col])\n",
    "\n",
    "neuron_profile = pd.DataFrame({\n",
    "    'neuron': u_names,\n",
    "    'brain_area': brain_areas,\n",
    "    'lateral': lateral,\n",
    "    'depth': depth,\n",
    "    **{f'{col}_beta': beta_dict[col] for col in X.columns},\n",
    "    **{f'{col}_pval': pval_dict[col] for col in X.columns}\n",
    "}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_profile.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "\n",
    "# Select only p-value columns\n",
    "pval_cols = [col for col in neuron_profile.columns if col.endswith('_pval')]\n",
    "\n",
    "# Feature matrix: rows = neurons, columns = p-values\n",
    "X_p = neuron_profile[pval_cols].values\n",
    "\n",
    "# Standardize for clustering\n",
    "scaler = StandardScaler()\n",
    "X_p_scaled = scaler.fit_transform(X_p)\n",
    "\n",
    "n_clusters = 3  # adjust as needed\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "p_clusters = kmeans.fit_predict(X_p_scaled)\n",
    "\n",
    "inertias = []\n",
    "sil_scores = []\n",
    "K_range = range(2, 10)  # try between 2 and 9 clusters\n",
    "\n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    labels = kmeans.fit_predict(X_p_scaled)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "    sil_scores.append(silhouette_score(X_p_scaled, labels))\n",
    "\n",
    "# Plot elbow curve\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(K_range, inertias, 'o-', color='blue')\n",
    "plt.xlabel('Number of clusters (k)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method')\n",
    "\n",
    "# Plot silhouette scores\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(K_range, sil_scores, 'o-', color='green')\n",
    "plt.xlabel('Number of clusters (k)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Analysis')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 5. Choose best k (example: highest silhouette score)\n",
    "best_k = K_range[int(np.argmax(sil_scores))]\n",
    "print(f\"Best number of clusters based on silhouette score: {best_k}\")\n",
    "\n",
    "# 6. Final clustering with best_k\n",
    "kmeans_final = KMeans(n_clusters=best_k, random_state=42, n_init=10)\n",
    "p_clusters = kmeans_final.fit_predict(X_p_scaled)\n",
    "\n",
    "# 7. Add cluster labels to DataFrame \n",
    "neuron_profile['p_cluster'] = p_clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Use the same scaled p-value matrix\n",
    "X = X_p_scaled  # already standardized\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], s=50)\n",
    "plt.title('PCA of Neuron P-Values')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define p-value threshold\n",
    "p_thresh = 0.01\n",
    "\n",
    "# Step 2: Identify all p-value columns\n",
    "pval_cols = [col for col in neuron_profile.columns if col.endswith('_pval')]\n",
    "\n",
    "# Step 3: Create new tuning columns based on significance\n",
    "for pval_col in pval_cols:\n",
    "    feature_name = pval_col.replace('_pval', '_tuning')\n",
    "    neuron_profile[feature_name] = (neuron_profile[pval_col] < p_thresh).astype(int)\n",
    "\n",
    "# Select all tuning columns\n",
    "tuning_cols = [col for col in neuron_profile.columns if col.endswith('_tuning')]\n",
    "tuning_cols = tuning_cols[1:]\n",
    "\n",
    "# Create label for each neuron's tuning pattern\n",
    "neuron_profile['tuning_pattern'] = neuron_profile[tuning_cols].apply(\n",
    "    lambda row: ','.join([col.replace('_tuning', '') for col in tuning_cols if row[col]]),\n",
    "    axis=1\n",
    ")\n",
    "# Convert boolean matrix to string patterns\n",
    "pattern_labels = neuron_profile[tuning_cols].apply(lambda row: ', '.join([col.replace('_tuning', '') for col in tuning_cols if row[col]]), axis=1)\n",
    "pattern_counts = pattern_labels.value_counts().reset_index()\n",
    "pattern_counts.columns = ['features_encoded', 'count']\n",
    "\n",
    "top_n = 15\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=pattern_counts.head(top_n), x='count', y='features_encoded', palette='mako')\n",
    "plt.xlabel('Number of Neurons')\n",
    "plt.ylabel('Encoded Features')\n",
    "plt.title(f'Top {top_n} Feature Encoding Combinations')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count top patterns\n",
    "top_patterns = neuron_profile['tuning_pattern'].value_counts().head(15).index.tolist()\n",
    "top_patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary: pattern → list of neuron indices\n",
    "pattern_to_plot = 'value,state_2,value_state_2'\n",
    "num_neurons = 5\n",
    "\n",
    "pattern_neuron_indices = {\n",
    "    pattern_to_plot: neuron_profile.index[neuron_profile['tuning_pattern'] == pattern_to_plot].tolist()\n",
    "}\n",
    "\n",
    "selected_neurons = random.sample(pattern_neuron_indices[pattern_to_plot], k=num_neurons)\n",
    "\n",
    "mean_FR_profile = pd.DataFrame(mean_FR, columns=[i for i in range(mean_FR.shape[1])])\n",
    "\n",
    "trial_info = trial_profile[['state', 'ch_val']].copy()\n",
    "\n",
    "# Set up plot grid: 3 rows (states) × 10 columns (neurons)\n",
    "fig, axes = plt.subplots(nrows=3, ncols=num_neurons, figsize=(25, 9), sharex=True, sharey=True)\n",
    "state_order = sorted(trial_info['state'].unique())\n",
    "\n",
    "for col_idx, neuron_id in enumerate(selected_neurons):\n",
    "    for row_idx, state in enumerate(state_order):\n",
    "        ax = axes[row_idx, col_idx]\n",
    "        state_df = trial_profile[trial_profile['state'] == state]\n",
    "        state_df['firing_rate'] = mean_FR_profile.loc[state_df.index, neuron_id]\n",
    "        \n",
    "        sns.scatterplot(\n",
    "            data=state_df,\n",
    "            x='ch_val',\n",
    "            y='firing_rate',\n",
    "            alpha=0.6,\n",
    "            ax=ax,\n",
    "            s=10\n",
    "        )\n",
    "\n",
    "        if row_idx == 0:\n",
    "            ax.set_title(f'Neuron {neuron_id}', fontsize=10)\n",
    "        if col_idx == 0:\n",
    "            ax.set_ylabel(f'State {state}', fontsize=10)\n",
    "        else:\n",
    "            ax.set_ylabel('')\n",
    "        ax.set_xlabel('')\n",
    "\n",
    "# Global labels\n",
    "fig.suptitle(f'Firing Rate vs Value for 10 {pattern_to_plot}-Tuned Neurons Across States', fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_profile.shape, mean_FR.shape, trial_profile.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_profile.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (\n",
    "    (neuron_profile['value_pval'] < p_thresh) &\n",
    "    (\n",
    "        (neuron_profile['state_1_pval'] < p_thresh) |\n",
    "        (neuron_profile['state_2_pval'] < p_thresh) |\n",
    "        (neuron_profile['state_3_pval'] < p_thresh)\n",
    "    )\n",
    ")\n",
    "\n",
    "# Get indices of those neurons\n",
    "dual_sig_indices = np.where(mask)[0]\n",
    "\n",
    "# Extract firing rates for those neurons\n",
    "firing = mean_FR[:, mask]  # shape: (n_trials, n_selected_neurons, n_timebins)\n",
    "\n",
    "plt.plot(firing)\n",
    "plt.title('Average Firing Rate of Value+State Tuned Neurons')\n",
    "plt.xlabel('Time (bins)')\n",
    "plt.ylabel('Firing Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the p-value columns for each regressor\n",
    "state_pvals = ['state_1_pval', 'state_2_pval', 'state_3_pval']\n",
    "value_pval = 'value_pval'\n",
    "interaction_pvals = ['value_state_1_pval', 'value_state_2_pval', 'value_state_3_pval']\n",
    "\n",
    "# Threshold for significance\n",
    "p_thresh = 0.01\n",
    "\n",
    "# State coding: any state term is significant\n",
    "state_coding = (\n",
    "    (neuron_profile[state_pvals[0]] < p_thresh) |\n",
    "    (neuron_profile[state_pvals[1]] < p_thresh) |\n",
    "    (neuron_profile[state_pvals[2]] < p_thresh)\n",
    ")\n",
    "\n",
    "# Value coding: value term is significant, but not state coding\n",
    "val_coding = (neuron_profile[value_pval] < p_thresh) & (~state_coding)\n",
    "\n",
    "# State-dependent value coding: any interaction term is significant, but not pure value coding\n",
    "state_val_coding = (\n",
    "    (neuron_profile[interaction_pvals[0]] < p_thresh) |\n",
    "    (neuron_profile[interaction_pvals[1]] < p_thresh) |\n",
    "    (neuron_profile[interaction_pvals[2]] < p_thresh)\n",
    ") & (~val_coding)\n",
    "\n",
    "# Refine state coding: exclude neurons already classified as state_val_coding\n",
    "state_coding = state_coding & (~state_val_coding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull out the factors\n",
    "state_coding = (t_factor_pvals[:,:,0] < .01) | (t_factor_pvals[:,:,1] < .01) | (t_factor_pvals[:,:,2] < .01)\n",
    "val_coding = (t_factor_pvals[:,:,3] < .01) & ~state_coding\n",
    "state_val_coding = (t_factor_pvals[:,:,4] < .01) | (t_factor_pvals[:,:,5] < .01) | (t_factor_pvals[:,:,6] < .01) & ~val_coding\n",
    "state_coding = state_coding & ~state_val_coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define significance thresholds\n",
    "sig_thresh = 0.05\n",
    "beta_thresh = 1e-3  # to exclude near-zero betas\n",
    "\n",
    "value_neurons = neuron_profile[\n",
    "    (neuron_profile['value_pval'] < sig_thresh) &\n",
    "    (neuron_profile['value_beta'].abs() > beta_thresh)\n",
    "].copy()\n",
    "\n",
    "# Use 1 for positive encoding, 0 for negative encoding\n",
    "value_neurons['encoding_sign'] = (value_neurons['value_beta'] > 0).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_profile.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_neurons.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_neurons.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Patch\n",
    "\n",
    "\n",
    "scatter = sns.scatterplot(\n",
    "    data=value_neurons,\n",
    "    x='lateral',\n",
    "    y='depth',\n",
    "    hue='encoding_sign',\n",
    "    palette={1: 'red', 0: 'blue'},\n",
    "    s=50,\n",
    "    edgecolor='black'\n",
    ")\n",
    "\n",
    "plt.title('Spatial Distribution of Value-Encoding Neurons')\n",
    "plt.xlabel('Lateral Position')\n",
    "plt.ylabel('Depth')\n",
    "plt.gca().invert_yaxis()  # optional: match anatomical orientation\n",
    "plt.legend(title='Value Encoding')\n",
    "\n",
    "legend_elements = [\n",
    "    Patch(facecolor='red', edgecolor='black', label='Positive Neurons'),\n",
    "    Patch(facecolor='blue', edgecolor='black', label='Negative Neurons')\n",
    "]\n",
    "plt.legend(handles=legend_elements, title='Value Encoding', loc='best')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_palette = {1: 'red', 0: 'blue'}\n",
    "\n",
    "# Filter data by brain area\n",
    "cdn_neurons = value_neurons[value_neurons['brain_area'] == 0]\n",
    "ofc_neurons = value_neurons[value_neurons['brain_area'] == 1]\n",
    "\n",
    "# Custom legend\n",
    "legend_elements = [\n",
    "    Patch(facecolor='red', edgecolor='black', label='Positive Neurons'),\n",
    "    Patch(facecolor='blue', edgecolor='black', label='Negative Neurons')\n",
    "]\n",
    "\n",
    "# --- Plot for CdN ---\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(\n",
    "    data=cdn_neurons,\n",
    "    x='lateral',\n",
    "    y='depth',\n",
    "    hue='encoding_sign',\n",
    "    palette=encoding_palette,\n",
    "    s=100,\n",
    "    edgecolor='black'\n",
    ")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.xlabel('Lateral Position')\n",
    "plt.ylabel('Depth')\n",
    "plt.title('CdN Value-Encoding Neurons')\n",
    "plt.legend(handles=legend_elements, title='Value Encoding')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Plot for OFC ---\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(\n",
    "    data=ofc_neurons,\n",
    "    x='lateral',\n",
    "    y='depth',\n",
    "    hue='encoding_sign',\n",
    "    palette=encoding_palette,\n",
    "    s=100,\n",
    "    edgecolor='black'\n",
    ")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.xlabel('Lateral Position')\n",
    "plt.ylabel('Depth')\n",
    "plt.title('OFC Value-Encoding Neurons')\n",
    "plt.legend(handles=legend_elements, title='Value Encoding')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your data directory and file list\n",
    "datadir = r\"D:\\Projects\\rotation_project\\reprocessed_data\"\n",
    "data_files = find_h5_files(datadir)\n",
    "data_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = r\"D:\\Projects\\rotation_project\\reprocessed_data\"\n",
    "data_files = find_h5_files(datadir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "elstonlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
